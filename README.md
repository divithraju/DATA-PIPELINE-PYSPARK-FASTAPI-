# ğŸš€ DATA-PIPELINE-PYSPARK-FASTAPI

A scalable **Data Pipeline and REST API project** combining **Apache PySpark** for distributed data processing with **FastAPI** for serving results via HTTP. This project demonstrates a real-world data engineering workflow: ingesting, transforming, and serving data, fully containerized for easy setup using Docker.

---

## ğŸ” Features

âœ”ï¸ Distributed Data Processing with PySpark
âœ”ï¸ REST API built with FastAPI (async & modern Python framework)
âœ”ï¸ Modular pipeline code under **pipeline/**
âœ”ï¸ API application under **app/**
âœ”ï¸ Automated tests under **tests/**
âœ”ï¸ Docker support for development & deployment

---

## ğŸ§° Tech Stack

| Component        | Technology              |
| ---------------- | ----------------------- |
| Data Engine      | Apache Spark (PySpark)  |
| Web API          | FastAPI                 |
| Language         | Python 3.x              |
| Containerization | Docker & Docker Compose |
| Testing          | PyTest / Unittest       |

---

## ğŸ“ Project Structure

```
DATA-PIPELINE-PYSPARK-FASTAPI/
â”œâ”€â”€ app/                       # FastAPI application
â”œâ”€â”€ pipeline/                  # PySpark data pipeline logic
â”œâ”€â”€ tests/                     # Automated tests
â”œâ”€â”€ Dockerfile                 # Docker image build instructions
â”œâ”€â”€ docker-compose.yml         # Docker Compose setup
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Project documentation
```

---

## ğŸ› ï¸ Installation & Setup

### ğŸ³ Using Docker (Recommended)

1. Clone the repository

```bash
git clone https://github.com/divithraju/DATA-PIPELINE-PYSPARK-FASTAPI-.git
cd DATA-PIPELINE-PYSPARK-FASTAPI-
```

2. Start Docker containers

```bash
docker-compose up --build
```

3. Open your browser

* FastAPI API: `http://localhost:8000/docs`
  (Swagger UI for testing endpoints)

---

## ğŸ§© Running Without Docker

### Backend (FastAPI)

```bash
cd app
python3 -m venv venv
source venv/bin/activate
pip install -r ../requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Visit `http://localhost:8000/docs`

### Pipeline (PySpark)

```bash
cd pipeline
spark-submit pipeline_script.py
```

*(Replace pipeline_script.py with your main PySpark script)*

---

## ğŸ“Œ API Endpoints (Sample)

| Method | Endpoint        | Description              |
| ------ | --------------- | ------------------------ |
| GET    | `/health`       | Check API status         |
| POST   | `/run_pipeline` | Trigger PySpark pipeline |
| GET    | `/data/results` | Fetch processed data     |

---

## ğŸ§ª Tests

```bash
pytest
```

---

## ğŸ“„ Why This Project Matters

âœ”ï¸ Real-world data engineering workflow
âœ”ï¸ FastAPI + PySpark integration
âœ”ï¸ Dockerized for quick deployment

---

## ğŸ‘¨â€ğŸ’» Author

**Divith Raju**
Backend & Data Engineering Enthusiast ğŸ‡®ğŸ‡³
GitHub: [https://github.com/divithraju](https://github.com/divithraju)

---

## ğŸ“ License

Open-source under the **MIT License**.

---

â­ *If you find this project useful, please give it a â­ on GitHub!*
